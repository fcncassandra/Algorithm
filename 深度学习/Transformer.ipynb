{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Transformer原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "李宏毅关于transformer的介绍：https://www.youtube.com/watch?v=ugWDIIOHtPA&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq：a1 a2 a3 a4 => b1 b2 b3 b4    \n",
    "如果是单向RNN，那么输出b3的时候，需要把a1 a2 a3都看过  \n",
    "如果是双向RNN，那么输出b3的时候，需要把a1 a2 a3 a4都看过，任何输出都要整个看过"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN计算不容易被并行化！    \n",
    "有人提出用CNN来取代RNN，如果有一堆filter，也可以输出一堆sequence  \n",
    "但是问题是每个CNN只能考虑非常有限的内容，可能每个只能考虑3个信息  \n",
    "但其实只要叠层很多，就可以获取更多的信息，但是如果第一层就想看很多信息就做不到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "self-attention想取代原来RNN要做的事情，这是一种新的layer，输入输出也是  \n",
    "每个输出也是看过整个input sequence，但是不同的是b1,b2,b3,b4是同时计算出来的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每一个a都有q,k,v  \n",
    "拿每个q去对每个k做attention，吃两个向量，然后输出一个分数，中间的方法有很多    \n",
    "这里是除以根号d，d是q和k的维度，不除以可能也没事：$\\alpha_{1,i} = q^1·k^i/\\sqrt{d}$  \n",
    "然后用softmax吸收所有的$\\alpha_{1,i}$，得到$\\hat{\\alpha}_{1,i}=exp(\\alpha_{1,i}) / \\sum_jexp(\\alpha_{1,j})$，这样其实就看到了全局的信息  \n",
    "让所有的$\\hat{\\alpha}_{1,i}$和$v^i$做加权求和，计算出$b^1 = \\sum_i\\hat{\\alpha}_{1,i}v^i$  \n",
    "所以这样可以并行计算出b1,b2,b3...  \n",
    "**天涯若比邻！** 如果不想看到远的东西，那么让远的v为0就可以"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还有一种变种，使用multi-head，好处可能是各司其职，每个head的关注点不一样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/multi-head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从attention来看，位置信息不是很重要  \n",
    "所以原始论文中要求把$a^i$加上$e^i$，每个不同位置都有一个，原来是特殊唯一的位置信息，用的是非常特殊的式子，而不是从数据中学习    \n",
    "为什么不是concat而不是相加？其实是可以的，但是论文中说了之前卷积的时候有人试过用concat效果不好    \n",
    "也有人尝试画过这个位置的图，大概就是下面这种，每个位置的向量都是唯一的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/position.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq中有一个encoder还有一个decoder，可以用来训练一个翻译器  \n",
    "都可以用self-attention layer来换掉RNN  \n",
    "decode的时候不仅会结合encode的部分，还会结合之前decode输出的部分！  \n",
    "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图中左边是encoder，右边是decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer norm是一个数据的不同维度的均值为0，标准差为1，一般搭配RNN使用   \n",
    "masked只会attend到已经产生的序列当中   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention可视化，是两个word之间就有一个attention，所以就像matrix，如果attention的权重越大就越粗  \n",
    "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果把tired换成wide，则发现这个it指向了street  \n",
    "如果用multi-head，发现两个head的关注点都不同"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有人用一堆文章去训练，想模型具备wikipedia的功能  \n",
    "输入的文章长度有的高达百万长度，如果用RNN可能直接烂掉 https://arxiv.org/abs/1801.10198\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/wiki_application.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有人发明了universal_transformer，就是在attention之后再用RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/universal_transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention机制也可以用于图像当中！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pics/transformer/gan.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
